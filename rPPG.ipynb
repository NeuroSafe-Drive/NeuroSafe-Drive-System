{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Clone the PhysFormer repository and prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'PhysFormer'...\n",
      "remote: Enumerating objects: 69, done.\u001b[K\n",
      "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
      "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
      "remote: Total 69 (delta 24), reused 8 (delta 0), pack-reused 7 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (69/69), 5.67 MiB | 1.78 MiB/s, done.\n",
      "--2024-08-15 18:13:29--  https://drive.usercontent.google.com/download?id=1n1TpMQfU-OkZdJglEJyFp-vGo9JXbgsT&export=download&authuser=0&confirm=t&uuid=6e3d51ce-76c0-43cc-8799-db0a79976c3a&at=APZUnTW_BrrV1iI05BG41_sccbpk%3A1710947798275\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.37.161, 2a00:1450:4006:811::2001\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.37.161|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 48611616 (46M) [application/octet-stream]\n",
      "Saving to: ‘download?id=1n1TpMQfU-OkZdJglEJyFp-vGo9JXbgsT&export=download&authuser=0&confirm=t&uuid=6e3d51ce-76c0-43cc-8799-db0a79976c3a&at=APZUnTW_BrrV1iI05BG41_sccbpk:1710947798275’\n",
      "\n",
      "download?id=1n1TpMQ 100%[===================>]  46.36M  3.40MB/s    in 14s     \n",
      "\n",
      "2024-08-15 18:13:45 (3.40 MB/s) - ‘download?id=1n1TpMQfU-OkZdJglEJyFp-vGo9JXbgsT&export=download&authuser=0&confirm=t&uuid=6e3d51ce-76c0-43cc-8799-db0a79976c3a&at=APZUnTW_BrrV1iI05BG41_sccbpk:1710947798275’ saved [48611616/48611616]\n",
      "\n",
      "mv: cannot stat '/content/download?id=1n1TpMQfU-OkZdJglEJyFp-vGo9JXbgsT&export=download&authuser=0&confirm=t&uuid=6e3d51ce-76c0-43cc-8799-db0a79976c3a&at=APZUnTW_BrrV1iI05BG41_sccbpk:1710947798275': No such file or directory\n",
      "unzip:  cannot find or open /content/v.zip, /content/v.zip.zip or /content/v.zip.ZIP.\n",
      "--2024-08-15 18:13:46--  https://drive.usercontent.google.com/download?id=1jBSbM88fA-beaoVi8ILFyL0SvVVMA9c9&export=download&authuser=0&confirm=t&uuid=d1551966-5490-4665-a90e-eb60816659ac&at=APZUnTUt_KWH-tRaMFc_eaBw1Hdq%3A1710948205447\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.37.161, 2a00:1450:4006:811::2001\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.37.161|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29759163 (28M) [application/octet-stream]\n",
      "Saving to: ‘download?id=1jBSbM88fA-beaoVi8ILFyL0SvVVMA9c9&export=download&authuser=0&confirm=t&uuid=d1551966-5490-4665-a90e-eb60816659ac&at=APZUnTUt_KWH-tRaMFc_eaBw1Hdq:1710948205447’\n",
      "\n",
      "download?id=1jBSbM8 100%[===================>]  28.38M  3.06MB/s    in 8.9s    \n",
      "\n",
      "2024-08-15 18:13:57 (3.18 MB/s) - ‘download?id=1jBSbM88fA-beaoVi8ILFyL0SvVVMA9c9&export=download&authuser=0&confirm=t&uuid=d1551966-5490-4665-a90e-eb60816659ac&at=APZUnTUt_KWH-tRaMFc_eaBw1Hdq:1710948205447’ saved [29759163/29759163]\n",
      "\n",
      "mv: cannot stat '/content/download?id=1jBSbM88fA-beaoVi8ILFyL0SvVVMA9c9&export=download&authuser=0&confirm=t&uuid=d1551966-5490-4665-a90e-eb60816659ac&at=APZUnTUt_KWH-tRaMFc_eaBw1Hdq:1710948205447': No such file or directory\n",
      "mv: cannot stat '/content/Physformer_VIPL_fold1.pkl': No such file or directory\n",
      "[Errno 2] No such file or directory: '/content/PhysFormer'\n",
      "/home/yara/Desktop/GP/NeuroSafe-Drive-System\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!git clone 'https://github.com/ZitongYu/PhysFormer.git'\n",
    "!wget 'https://drive.usercontent.google.com/download?id=1n1TpMQfU-OkZdJglEJyFp-vGo9JXbgsT&export=download&authuser=0&confirm=t&uuid=6e3d51ce-76c0-43cc-8799-db0a79976c3a&at=APZUnTW_BrrV1iI05BG41_sccbpk%3A1710947798275'\n",
    "!mv '/content/download?id=1n1TpMQfU-OkZdJglEJyFp-vGo9JXbgsT&export=download&authuser=0&confirm=t&uuid=6e3d51ce-76c0-43cc-8799-db0a79976c3a&at=APZUnTW_BrrV1iI05BG41_sccbpk:1710947798275' 'v.zip'\n",
    "!unzip '/content/v.zip'\n",
    "!wget 'https://drive.usercontent.google.com/download?id=1jBSbM88fA-beaoVi8ILFyL0SvVVMA9c9&export=download&authuser=0&confirm=t&uuid=d1551966-5490-4665-a90e-eb60816659ac&at=APZUnTUt_KWH-tRaMFc_eaBw1Hdq%3A1710948205447'\n",
    "!mv '/content/download?id=1jBSbM88fA-beaoVi8ILFyL0SvVVMA9c9&export=download&authuser=0&confirm=t&uuid=d1551966-5490-4665-a90e-eb60816659ac&at=APZUnTUt_KWH-tRaMFc_eaBw1Hdq:1710948205447' 'Physformer_VIPL_fold1.pkl'\n",
    "!mv '/content/Physformer_VIPL_fold1.pkl' '/content/PhysFormer'\n",
    "%cd '/content/PhysFormer'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the required libraries and the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'vision'...\n",
      "remote: Enumerating objects: 560571, done.\u001b[K\n",
      "remote: Counting objects: 100% (8357/8357), done.\u001b[K\n",
      "remote: Compressing objects: 100% (564/564), done.\u001b[K\n",
      "^Cceiving objects:  13% (72875/560571), 128.75 MiB | 2.63 MiB/s\n",
      "/bin/bash: line 0: cd: vision: No such file or directory\n",
      "/bin/bash: python: command not found\n"
     ]
    }
   ],
   "source": [
    "# Clone the torchvision repository\n",
    "!git clone https://github.com/pytorch/vision.git\n",
    "!cd vision\n",
    "\n",
    "# Install torchvision from source\n",
    "!python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ViT_ST_ST_Compact3_TDC_gra_sharp\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodulefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/_meta_registrations.py:26\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroi_align\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[1;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from model import ViT_ST_ST_Compact3_TDC_gra_sharp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = ViT_ST_ST_Compact3_TDC_gra_sharp(image_size=(160, 128, 128), patches=(4, 4, 4), dim=96, ff_dim=144, num_heads=4, num_layers=12, dropout_rate=0.1, theta=0.7)\n",
    "pytorch_model = pytorch_model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pytorch_model.load_state_dict(torch.load('Physformer_VIPL_fold1.pkl'))  \n",
    "pytorch_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Prepare a sample video for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (128, 160))  \n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "# Example usage with a sample video file\n",
    "sample_video = preprocess_video('path_to_sample_video.mp4')  # Provide the path to your sample video\n",
    "sample_video_tensor = torch.from_numpy(sample_video).float().cuda()\n",
    "\n",
    "# Run inference on the sample video\n",
    "with torch.no_grad():\n",
    "    output = pytorch_model(sample_video_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
