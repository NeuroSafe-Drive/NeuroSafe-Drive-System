{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Clone the PhysFormer repository and prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!git clone 'https://github.com/ZitongYu/PhysFormer.git'\n",
    "!wget 'https://drive.usercontent.google.com/download?id=1n1TpMQfU-OkZdJglEJyFp-vGo9JXbgsT&export=download&authuser=0&confirm=t&uuid=6e3d51ce-76c0-43cc-8799-db0a79976c3a&at=APZUnTW_BrrV1iI05BG41_sccbpk%3A1710947798275'\n",
    "!mv '/content/download?id=1n1TpMQfU-OkZdJglEJyFp-vGo9JXbgsT&export=download&authuser=0&confirm=t&uuid=6e3d51ce-76c0-43cc-8799-db0a79976c3a&at=APZUnTW_BrrV1iI05BG41_sccbpk:1710947798275' 'v.zip'\n",
    "!unzip '/content/v.zip'\n",
    "!wget 'https://drive.usercontent.google.com/download?id=1jBSbM88fA-beaoVi8ILFyL0SvVVMA9c9&export=download&authuser=0&confirm=t&uuid=d1551966-5490-4665-a90e-eb60816659ac&at=APZUnTUt_KWH-tRaMFc_eaBw1Hdq%3A1710948205447'\n",
    "!mv '/content/download?id=1jBSbM88fA-beaoVi8ILFyL0SvVVMA9c9&export=download&authuser=0&confirm=t&uuid=d1551966-5490-4665-a90e-eb60816659ac&at=APZUnTUt_KWH-tRaMFc_eaBw1Hdq:1710948205447' 'Physformer_VIPL_fold1.pkl'\n",
    "!mv '/content/Physformer_VIPL_fold1.pkl' '/content/PhysFormer'\n",
    "%cd '/content/PhysFormer'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the required libraries and the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from model import ViT_ST_ST_Compact3_TDC_gra_sharp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_model = ViT_ST_ST_Compact3_TDC_gra_sharp(image_size=(160, 128, 128), patches=(4, 4, 4), dim=96, ff_dim=144, num_heads=4, num_layers=12, dropout_rate=0.1, theta=0.7)\n",
    "pytorch_model = pytorch_model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pytorch_model.load_state_dict(torch.load('Physformer_VIPL_fold1.pkl'))  \n",
    "pytorch_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Prepare a sample video for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (128, 160))  \n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return np.array(frames)\n",
    "\n",
    "# Example usage with a sample video file\n",
    "sample_video = preprocess_video('path_to_sample_video.mp4')  # Provide the path to your sample video\n",
    "sample_video_tensor = torch.from_numpy(sample_video).float().cuda()\n",
    "\n",
    "# Run inference on the sample video\n",
    "with torch.no_grad():\n",
    "    output = pytorch_model(sample_video_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
